{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Autoencoding Gaussian Mixture Model for Unsupervised Anomaly Detection - Adversarial Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from data_loader import *\n",
    "from main import *\n",
    "import time\n",
    "from IPython.utils import io\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './data'\n",
    "num_examples_per_attack = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class hyperparams():\n",
    "    def __init__(self, config):\n",
    "        self.__dict__.update(**config)\n",
    "defaults = {\n",
    "    'lr' : 1e-4,\n",
    "    'num_epochs' : 2,\n",
    "    'batch_size' : 1024,\n",
    "    'gmm_k' : 4,\n",
    "    'lambda_energy' : 0.1,\n",
    "    'lambda_cov_diag' : 0.005,\n",
    "    'pretrained_model' : None,\n",
    "    'mode' : 'train_only',\n",
    "    'use_tensorboard' : False,\n",
    "    'data_path' : data_path,\n",
    "    'ds': 'cicids2017',\n",
    "\n",
    "    'log_path' : './dagmm/logs',\n",
    "    'model_save_path' : './dagmm/models',\n",
    "    'sample_path' : './dagmm/samples',\n",
    "    'test_sample_path' : './dagmm/test_samples',\n",
    "    'result_path' : './dagmm/results',\n",
    "\n",
    "    'log_step' : None,\n",
    "    'sample_step' : 194,\n",
    "    'model_save_step' : 194,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DaGMM\n",
      "DaGMM(\n",
      "  (encoder): Sequential(\n",
      "    (0): Linear(in_features=77, out_features=60, bias=True)\n",
      "    (1): Tanh()\n",
      "    (2): Linear(in_features=60, out_features=30, bias=True)\n",
      "    (3): Tanh()\n",
      "    (4): Linear(in_features=30, out_features=10, bias=True)\n",
      "    (5): Tanh()\n",
      "    (6): Linear(in_features=10, out_features=1, bias=True)\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): Linear(in_features=1, out_features=10, bias=True)\n",
      "    (1): Tanh()\n",
      "    (2): Linear(in_features=10, out_features=30, bias=True)\n",
      "    (3): Tanh()\n",
      "    (4): Linear(in_features=30, out_features=60, bias=True)\n",
      "    (5): Tanh()\n",
      "    (6): Linear(in_features=60, out_features=77, bias=True)\n",
      "  )\n",
      "  (estimation): Sequential(\n",
      "    (0): Linear(in_features=3, out_features=10, bias=True)\n",
      "    (1): Tanh()\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=10, out_features=4, bias=True)\n",
      "    (4): Softmax(dim=1)\n",
      "  )\n",
      ")\n",
      "The number of parameters: 13822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 518/518 [00:09<00:00, 52.63it/s]\n",
      "100%|██████████| 518/518 [00:09<00:00, 52.25it/s]\n"
     ]
    }
   ],
   "source": [
    "solver = main(hyperparams(defaults))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inferência"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constantes do teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### At this threshold the FPR of the model we trained is 0.1\n",
    "real_thr = 0.01653931848704815\n",
    "thr = real_thr*0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ./data/flow_based/Thursday-WH-generate-labeled.csv\n",
      "1 ./data/flow_based/Tuesday-WH-generate-labeled.csv\n"
     ]
    }
   ],
   "source": [
    "# test_files = [data_path+'/flow_based/Tuesday-WH-generate-labeled.csv',\n",
    "#             data_path+'/flow_based/Wednesday-WH-generate-labeled.csv',\n",
    "#             data_path+'/flow_based/Thursday-WH-generate-labeled.csv',\n",
    "#             data_path+'/flow_based/Friday-WH-generate-labeled.csv']\n",
    "\n",
    "test_files = [data_path+'/flow_based/Thursday-WH-generate-labeled.csv',\n",
    "                data_path+'/flow_based/Tuesday-WH-generate-labeled.csv']\n",
    "\n",
    "train_min = np.load(data_path+'/flow_based/x_train_meta/train_min.npy')\n",
    "train_max = np.load(data_path+'/flow_based/x_train_meta/train_max.npy')\n",
    "\n",
    "x_test_all = []\n",
    "y_test_all = []\n",
    "all_label_set = []\n",
    "for i in range(len(test_files)):\n",
    "    print (i,test_files[i])\n",
    "    url_data = test_files[i]\n",
    "    df = pd.read_csv(url_data)\n",
    "\n",
    "    feats = df.iloc[:,8:]\n",
    "    ds_port = df.iloc[:,5]\n",
    "    df = pd.concat([ds_port,feats],axis=1)\n",
    "\n",
    "    labels = df.iloc[:,-1].values\n",
    "    label_set = set(labels)\n",
    "    all_label_set.append(label_set)\n",
    "\n",
    "    all_feats = df.iloc[:,:-1].astype(np.float64).values\n",
    "    known_data_IDs =(np.any(np.isinf(all_feats),axis=1) + np.any(np.isnan(all_feats),axis=1))==False\n",
    "    x_test = all_feats[known_data_IDs]\n",
    "    y_test = df.iloc[:,-1].values\n",
    "    y_test = y_test[known_data_IDs]\n",
    "#         x_test = (x_test - train_min)/(train_max - train_min+1e-6)\n",
    "    x_test_all.append(x_test)\n",
    "    y_test_all.append(y_test)\n",
    "x_test = np.concatenate(x_test_all,axis=0).astype(np.float32)\n",
    "y_test = np.concatenate(y_test_all,axis=0)\n",
    "\n",
    "#### list of features which are decimal:\n",
    "decimal_features = []\n",
    "for i in range(x_test.shape[1]):\n",
    "    a1 = x_test[:,i]\n",
    "    a2 = np.round(a1)\n",
    "    temp = np.sum(np.abs(a1-a2))\n",
    "    if temp==0:\n",
    "#             print (i,df.columns[i])\n",
    "        decimal_features.append(i)\n",
    "\n",
    "num_input = x_test.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = tf.Variable(np.zeros((1, x_test.shape[1]), dtype=np.float32),name='modifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_mask = np.ones(77,dtype=np.float32)\n",
    "\n",
    "######### Group 1 features (based on the categorization in  \"Towards Evaluation of NIDSs in Adversarial Setting\") ######### \n",
    "fixed_mask[0] = 0 #dst_port\n",
    "fixed_mask[3] = 0 #bwd\n",
    "fixed_mask[4] = 0 #total len of fwd pkts (sum of all payloads in fwd direction)\n",
    "fixed_mask[5] = 0 #total len of bwd pkts (sum of all payloads in bwd direction)\n",
    "fixed_mask[10] = 0 #bwd\n",
    "fixed_mask[11] = 0 #bwd\n",
    "fixed_mask[12] = 0 #bwd\n",
    "fixed_mask[13] = 0 #bwd\n",
    "fixed_mask[25] = 0 #bwd\n",
    "fixed_mask[26] = 0 #bwd\n",
    "fixed_mask[27] = 0 #bwd\n",
    "fixed_mask[28] = 0 #bwd\n",
    "fixed_mask[29] = 0 #bwd\n",
    "fixed_mask[31] = 0 #bwd\n",
    "fixed_mask[33] = 0 #bwd\n",
    "fixed_mask[35] = 0 #bwd\n",
    "fixed_mask[37] = 0 #bwd\n",
    "fixed_mask[53] = 0 \n",
    "fixed_mask[54] = 0 #bwd\n",
    "for i in range(58,65):\n",
    "    fixed_mask[i]=0\n",
    "fixed_mask[66] = 0 #bwd\n",
    "\n",
    "\n",
    "\n",
    "######### Group 4 features (based on the categorization in  \"Towards Evaluation of NIDSs in Adversarial Setting\") ######### \n",
    "fixed_mask[9] = 0 #std (fwd-payload)\n",
    "fixed_mask[17] = 0 #std\n",
    "fixed_mask[22] = 0 #std\n",
    "fixed_mask[41] = 0 #std\n",
    "fixed_mask[42] = 0 #var\n",
    "fixed_mask[55] = 0 #bulk\n",
    "fixed_mask[56] = 0 #bulk\n",
    "fixed_mask[57] = 0 #bulk\n",
    "for i in range(68,77):\n",
    "    fixed_mask[i]=0\n",
    "    \n",
    "\n",
    "    \n",
    "################## Group 3 (Dependent) features (based on the categorization in  \"Towards Evaluation of NIDSs in Adversarial Setting\") ######### \n",
    "depended_features = {8,14,15,16,20,21,34,36,38,39,40,51,52,67}\n",
    "for i in depended_features:\n",
    "    fixed_mask[i] = 0\n",
    "    \n",
    "    \n",
    "################## Other features are Group 2 (Independent) features\n",
    "\n",
    "\n",
    "mask_l1 = np.copy(fixed_mask)\n",
    "mask_l2 = np.copy(fixed_mask)\n",
    "####unmask stds\n",
    "mask_l2[9]=1\n",
    "mask_l2[17]=1\n",
    "mask_l2[22]=1\n",
    "mask_l2[41]=1\n",
    "mask_l2[42]=1\n",
    "mask_l3 = np.copy(mask_l2)\n",
    "mask_l3[55] = 1 #bulk\n",
    "mask_l3[56] = 1 #bulk\n",
    "mask_l3[57] = 1 #bulk\n",
    "for i in range(68,77):\n",
    "    mask_l3[i]=1\n",
    "\n",
    "\n",
    "#### The duplicated features:\n",
    "dup_ht={61:2,\n",
    "        63:3,\n",
    "        62:4,\n",
    "        64:5}\n",
    "mins = [7,19,24,38]\n",
    "maxs = [6,18,23,39]\n",
    "flags = [30,32,43,44,45,46,47,48,49,50]\n",
    "aggr_features=[9,17,22,41,42,55,56,57,68,69,70,71,72,73,74,75,76]\n",
    "\n",
    "\n",
    "optimizer_001 = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "optimizer_01 = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "optimizer_1 = tf.keras.optimizers.Adam(learning_rate=0.1)\n",
    "optimizer_10 = tf.keras.optimizers.Adam(learning_rate=1.)\n",
    "all_optimizers = [optimizer_001,optimizer_01,optimizer_1,optimizer_10]\n",
    "for op in all_optimizers:\n",
    "    op.apply_gradients(zip([alpha],[alpha]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funções auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x):\n",
    "    x_normalized =(x - train_min)/(train_max - train_min+1e-6)\n",
    "    return x_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(x):\n",
    "    with io.capture_output() as captured:\n",
    "        score = solver.test([normalize(x).astype(np.float32)], energy_only=True)[0]\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def get_adv(x,adv_mask):\n",
    "    x_normalized = (x - train_min)/(train_max - train_min+1e-6)\n",
    "    alpha_masked = alpha*adv_mask\n",
    "    adv_ex_normalized = x_normalized + alpha_masked\n",
    "    adv_ex = adv_ex_normalized*(train_max - train_min + 1e-6) + train_min #### unnormalized\n",
    "    return adv_ex\n",
    "\n",
    "@tf.function\n",
    "def optim(x,adv_mask,optimizer):\n",
    "    with tf.GradientTape() as tape:\n",
    "        x_normalized = (x - train_min)/(train_max - train_min+1e-6)\n",
    "        alpha_masked = alpha*adv_mask\n",
    "        adv_ex_normalized = x_normalized + alpha_masked\n",
    "\n",
    "        # def_mask = tf.random.uniform(shape=[1*100,num_input])\n",
    "        # def_mask = tf.cast((def_mask>0.75),tf.float32)\n",
    "        # partial_x = def_mask*adv_ex_normalized\n",
    "        # rec_x = model(partial_x, training=False)\n",
    "\n",
    "        # score = tf.reduce_mean(tf.square(rec_x - adv_ex_normalized),axis=1)\n",
    "        # score = tf.reduce_sum(score)\n",
    "        score = get_score(adv_ex_normalized)\n",
    "        loss = score\n",
    "\n",
    "    gradients = tape.gradient(loss, [alpha])\n",
    "    optimizer.apply_gradients(zip(gradients, [alpha]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_adv(source_index, x_test_mal, mal_counter):\n",
    "    alpha.assign(np.zeros(alpha.shape))\n",
    "    ##### CHECK TO SEE IF IT NEEDS TO BE ADVERSARIALLY CHANGED #####\n",
    "    orig_sample = np.copy(x_test_mal[source_index][None])\n",
    "    adv_ex = get_adv(orig_sample,mask_l1)\n",
    "    sc = get_score(adv_ex)\n",
    "    sc = sc.numpy()\n",
    "    if sc<thr:\n",
    "        return 'no change'\n",
    "    mal_counter[0]+=1\n",
    "    backup_adv = [None]\n",
    "    def optimize(optimizer,total_iter,mask_v):\n",
    "        alpha.assign(np.zeros(alpha.shape))\n",
    "        for i in range(total_iter):\n",
    "            optim(orig_sample,mask_v,optimizer)\n",
    "            adv_ex = get_adv(orig_sample,mask_v)\n",
    "            adv_ex_np = adv_ex.numpy()\n",
    "            for k in dup_ht:\n",
    "                adv_ex_np[0,k] = adv_ex_np[0,dup_ht[k]]\n",
    "            adv_ex_np[0,mins] = np.maximum(0,adv_ex_np[0,mins])\n",
    "            adv_ex_np[0,mins] = np.minimum(orig_sample[0,mins],adv_ex_np[0,mins])\n",
    "            adv_ex_np[0,maxs] = np.maximum(orig_sample[0,maxs],adv_ex_np[0,maxs])\n",
    "            adv_ex_np[0,flags] = np.maximum(orig_sample[0,flags],adv_ex_np[0,flags])\n",
    "            flags_max_changed = np.max(adv_ex_np[0,flags] - orig_sample[0,flags])\n",
    "            adv_ex_np[0,aggr_features] = np.maximum(0,adv_ex_np[0,aggr_features])\n",
    "            adv_ex_np[0,2] = np.maximum(orig_sample[0,2]+flags_max_changed,adv_ex_np[0,2])\n",
    "            adv_ex_np[0,65] = np.maximum(0,adv_ex_np[0,65])\n",
    "            \n",
    "            ##### round the ones that should be rounded #### \n",
    "            adv_ex_np[0,decimal_features] = np.round(adv_ex_np[0,decimal_features])\n",
    "            ##################### recalculate dependent features ######################\n",
    "            adv_ex_np[0,4] = adv_ex_np[0,4] + (adv_ex_np[0,6]!=orig_sample[0,6])*adv_ex_np[0,6] + (adv_ex_np[0,7]!=orig_sample[0,7])*adv_ex_np[0,7]\n",
    "            adv_ex_np[0,8] = adv_ex_np[0,4]/adv_ex_np[0,2]\n",
    "            adv_ex_np[0,14]=(adv_ex_np[0,4]+adv_ex_np[0,5])/adv_ex_np[0,1]*1e6\n",
    "            adv_ex_np[0,15]=(adv_ex_np[0,2]+adv_ex_np[0,3])/adv_ex_np[0,1]*1e6\n",
    "            adv_ex_np[0,16]=adv_ex_np[0,1]/(adv_ex_np[0,2]+adv_ex_np[0,3]-1)\n",
    "            \n",
    "            adv_ex_np[0,21]=adv_ex_np[0,20]/(adv_ex_np[0,2]-1)\n",
    "            adv_ex_np[0,34]=adv_ex_np[0,34] + 20*(adv_ex_np[0,2] - orig_sample[0,2])\n",
    "            adv_ex_np[0,36]=adv_ex_np[0,2]/adv_ex_np[0,1]*1e6\n",
    "            \n",
    "            adv_ex_np[0,38]=np.minimum(adv_ex_np[0,38],adv_ex_np[0,7])\n",
    "            adv_ex_np[0,39]=np.maximum(adv_ex_np[0,39],adv_ex_np[0,6])\n",
    "            \n",
    "            adv_ex_np[0,40]=(adv_ex_np[0,4]+adv_ex_np[0,5])/(adv_ex_np[0,3]+adv_ex_np[0,2]+1)\n",
    "            adv_ex_np[0,51]=adv_ex_np[0,3]/adv_ex_np[0,2]\n",
    "            adv_ex_np[0,52]=(adv_ex_np[0,4]+adv_ex_np[0,5])/(adv_ex_np[0,3]+adv_ex_np[0,2])\n",
    "            adv_ex_np[0,53]=adv_ex_np[0,8]\n",
    "            adv_ex_np[0,67]=adv_ex_np[0,67] + (adv_ex_np[0,6]!=orig_sample[0,6])*1 + (adv_ex_np[0,7]!=orig_sample[0,7])*1\n",
    "            \n",
    "            adv_ex_np[np.isinf(adv_ex_np)]=0\n",
    "            adv_ex_np[np.isnan(adv_ex_np)]=0\n",
    "            sc = get_score(adv_ex_np)\n",
    "            sc = sc.numpy()\n",
    "            if sc<thr:\n",
    "                return adv_ex_np\n",
    "            if sc<real_thr and isinstance(backup_adv[0],type(None)):\n",
    "                backup_adv[0] = adv_ex_np\n",
    "                \n",
    "        return None\n",
    "\n",
    "    for mask in [mask_l1,mask_l2,mask_l3]:\n",
    "        res = optimize(optimizer_001,30,mask)\n",
    "        if isinstance(res,type(None)):\n",
    "            res = optimize(optimizer_01,40,mask)\n",
    "            if isinstance(res,type(None)):\n",
    "                res = optimize(optimizer_1,50,mask)\n",
    "                if isinstance(res,type(None)):\n",
    "                    res = optimize(optimizer_10,60,mask)\n",
    "        if isinstance(res,type(None))==False:\n",
    "            break\n",
    "    if isinstance(res,type(None)):\n",
    "        return backup_adv[0]\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemplo adversarial por tipo de ataque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_names = ['FTP-Patator','SSH-Patator','DoS slowloris','DoS Slowhttptest','DoS Hulk','DoS GoldenEye',\n",
    "#           'Heartbleed','Web Attack','Infiltration', 'Bot', 'PortScan', 'DDoS']\n",
    "label_names = ['FTP-Patator']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####################BENIGN######################\n",
      "BENIGN (5, 77)\n",
      "TPR of the attacker's local copy of the NIDS: 0.0000\n",
      "TPR of the victim's NIDS: 0.0000\n",
      "#####################FTP-Patator######################\n",
      "FTP-Patator (5, 77)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20337/4260252449.py:35: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  adv_ex_np[0,21]=adv_ex_np[0,20]/(adv_ex_np[0,2]-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TPR of the attacker's local copy of the NIDS: 0.4000\n",
      "TPR of the victim's NIDS: 0.4000\n",
      "#####################SSH-Patator######################\n",
      "SSH-Patator (5, 77)\n",
      "TPR of the attacker's local copy of the NIDS: 0.0000\n",
      "TPR of the victim's NIDS: 0.0000\n",
      "#####################DoS slowloris######################\n",
      "#####################DoS Slowhttptest######################\n",
      "#####################DoS Hulk######################\n",
      "#####################DoS GoldenEye######################\n",
      "#####################Heartbleed######################\n",
      "#####################Web Attack######################\n",
      "#####################Infiltration######################\n",
      "Infiltration (5, 77)\n",
      "TPR of the attacker's local copy of the NIDS: 1.0000\n",
      "TPR of the victim's NIDS: 1.0000\n",
      "#####################Bot######################\n",
      "#####################PortScan######################\n",
      "#####################DDoS######################\n"
     ]
    }
   ],
   "source": [
    "for attack_type in label_names:\n",
    "\n",
    "    print(\"#####################\" + attack_type + \"######################\")\n",
    "\n",
    "    x_test_mal = x_test[y_test==attack_type]\n",
    "    if(len(x_test_mal) != 0):\n",
    "        x_test_mal = x_test_mal[:num_examples_per_attack].astype(np.float32)\n",
    "        x_test_adv = np.zeros_like(x_test_mal)\n",
    "        mal_counter = [0]\n",
    "        cons_as_mal = 0\n",
    "        cons_as_ben = 0\n",
    "        fooled = 0\n",
    "        st = time.time()\n",
    "        print (attack_type, x_test_mal.shape)\n",
    "        for i in range(len(x_test_mal)):\n",
    "            res = find_adv(i, x_test_mal, mal_counter)\n",
    "            # print(\"RES:\", res)\n",
    "            if isinstance(res, str) and res =='no change':\n",
    "                cons_as_ben+=1\n",
    "                x_test_adv[i] = np.copy(x_test_mal[i].astype(np.float32))\n",
    "            elif isinstance(res,type(None)):\n",
    "                cons_as_mal+=1\n",
    "                x_test_adv[i] = np.copy(x_test_mal[i].astype(np.float32))\n",
    "            else:\n",
    "                fooled+=1\n",
    "                x_test_adv[i] = res\n",
    "\n",
    "        print (\"TPR of the attacker's local copy of the NIDS: {0:0.4f}\".format(cons_as_mal/len(x_test_mal)))\n",
    "\n",
    "\n",
    "        score_np2 = np.zeros(len(x_test_adv))\n",
    "        begin_time = time.time()\n",
    "        for i in range(len(x_test_adv)):\n",
    "            sample = x_test_adv[i][None]\n",
    "            score_temp = get_score(sample)\n",
    "            score_np2[i] = score_temp.numpy()\n",
    "        mal_scores = score_np2\n",
    "        print (\"TPR of the victim's NIDS: {0:0.4f}\".format(np.sum(mal_scores>=thr)/(0. + len(mal_scores))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pykan-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
